
pytorch	2.9.1+cpu:€¨
›
input
features.0.0.weight
features.0.0.weight_biasgetitemnode_Conv_715"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≠
	namespaceü: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.0: torchvision.ops.misc.Conv2dNormActivation/features.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJÜ
pkg.torch.onnx.class_hierarchy„['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J∏
pkg.torch.onnx.fx_nodeù%_native_batch_norm_legit_no_training : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d, %p_features_0_1_weight, %p_features_0_1_bias, %b_features_0_1_running_mean, %b_features_0_1_running_var, 0.1, 1e-05), kwargs = {})Jt
pkg.torch.onnx.name_scopesV['', 'features', 'features.0', 'features.0.1', '_native_batch_norm_legit_no_training']JÊ
pkg.torch.onnx.stack_trace«File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
û
getitem
min_val_cast
max_val_casthardtanhn4"ClipJ
	namespace‚: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.0: torchvision.ops.misc.Conv2dNormActivation/features.0.2: torch.nn.modules.activation.ReLU6/hardtanh: aten.hardtanh.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jï
pkg.torch.onnx.fx_node{%hardtanh : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem, 0.0, 6.0), kwargs = {})JX
pkg.torch.onnx.name_scopes:['', 'features', 'features.0', 'features.0.2', 'hardtanh']Jï
pkg.torch.onnx.stack_traceˆFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
É
hardtanh
features.1.conv.0.0.weight
features.1.conv.0.0.weight_bias	getitem_3node_Conv_717"Conv*
group †*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.1.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_1: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_1 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_1, %p_features_1_conv_0_1_weight, %p_features_1_conv_0_1_bias, %b_features_1_conv_0_1_running_mean, %b_features_1_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.0', 'features.1.conv.0.1', '_native_batch_norm_legit_no_training_1']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ö
	getitem_3
min_val_cast
max_val_cast
hardtanh_1n4_2"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.1.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_1: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jô
pkg.torch.onnx.fx_node%hardtanh_1 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_3, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.0', 'features.1.conv.0.2', 'hardtanh_1']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ö

hardtanh_1
features.1.conv.1.weight
features.1.conv.1.weight_bias	getitem_6node_Conv_719"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.2: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_2: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_2 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_2, %p_features_1_conv_2_weight, %p_features_1_conv_2_bias, %b_features_1_conv_2_running_mean, %b_features_1_conv_2_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.2', '_native_batch_norm_legit_no_training_2']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Ñ
	getitem_6
features.2.conv.0.0.weight
features.2.conv.0.0.weight_bias	getitem_9node_Conv_721"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_3: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_3 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_3, %p_features_2_conv_0_1_weight, %p_features_2_conv_0_1_bias, %b_features_2_conv_0_1_running_mean, %b_features_2_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.0', 'features.2.conv.0.1', '_native_batch_norm_legit_no_training_3']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ö
	getitem_9
min_val_cast
max_val_cast
hardtanh_2n4_3"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_2: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jô
pkg.torch.onnx.fx_node%hardtanh_2 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_9, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.0', 'features.2.conv.0.2', 'hardtanh_2']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ü

hardtanh_2
features.2.conv.1.0.weight
features.2.conv.1.0.weight_bias
getitem_12node_Conv_723"Conv*
group`†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_4: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_4 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_4, %p_features_2_conv_1_1_weight, %p_features_2_conv_1_1_bias, %b_features_2_conv_1_1_running_mean, %b_features_2_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.1', 'features.2.conv.1.1', '_native_batch_norm_legit_no_training_4']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_12
min_val_cast
max_val_cast
hardtanh_3n4_4"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_3: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_3 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_12, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.1', 'features.2.conv.1.2', 'hardtanh_3']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ü

hardtanh_3
features.2.conv.2.weight
features.2.conv.2.weight_bias
getitem_15node_Conv_725"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_5: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_5 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_5, %p_features_2_conv_3_weight, %p_features_2_conv_3_bias, %b_features_2_conv_3_running_mean, %b_features_2_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.3', '_native_batch_norm_legit_no_training_5']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ä

getitem_15
features.3.conv.0.0.weight
features.3.conv.0.0.weight_bias
getitem_18node_Conv_727"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_6: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_6 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_6, %p_features_3_conv_0_1_weight, %p_features_3_conv_0_1_bias, %b_features_3_conv_0_1_running_mean, %b_features_3_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.0', 'features.3.conv.0.1', '_native_batch_norm_legit_no_training_6']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_18
min_val_cast
max_val_cast
hardtanh_4n4_5"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_4: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_4 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_18, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.0', 'features.3.conv.0.2', 'hardtanh_4']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ã

hardtanh_4
features.3.conv.1.0.weight
features.3.conv.1.0.weight_bias
getitem_21node_Conv_729"Conv*
groupê†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_7: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_7 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_7, %p_features_3_conv_1_1_weight, %p_features_3_conv_1_1_bias, %b_features_3_conv_1_1_running_mean, %b_features_3_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.1', 'features.3.conv.1.1', '_native_batch_norm_legit_no_training_7']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_21
min_val_cast
max_val_cast
hardtanh_5n4_6"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_5: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_5 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_21, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.1', 'features.3.conv.1.2', 'hardtanh_5']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ä

hardtanh_5
features.3.conv.2.weight
features.3.conv.2.weight_bias
getitem_24node_Conv_731"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_8: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_8 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_8, %p_features_3_conv_3_weight, %p_features_3_conv_3_bias, %b_features_3_conv_3_running_mean, %b_features_3_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.3', '_native_batch_norm_legit_no_training_8']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_15

getitem_24add_5
node_add_5"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/add_5: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jí
pkg.torch.onnx.fx_nodex%add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_15, %getitem_24), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.3', 'add_5']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
Å
add_5
features.4.conv.0.0.weight
features.4.conv.0.0.weight_bias
getitem_27node_Conv_733"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_9: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_9 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_9, %p_features_4_conv_0_1_weight, %p_features_4_conv_0_1_bias, %b_features_4_conv_0_1_running_mean, %b_features_4_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.0', 'features.4.conv.0.1', '_native_batch_norm_legit_no_training_9']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_27
min_val_cast
max_val_cast
hardtanh_6n4_7"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_6: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_6 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_27, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.0', 'features.4.conv.0.2', 'hardtanh_6']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ã

hardtanh_6
features.4.conv.1.0.weight
features.4.conv.1.0.weight_bias
getitem_30node_Conv_735"Conv*
groupê†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_10: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_10 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_10, %p_features_4_conv_1_1_weight, %p_features_4_conv_1_1_bias, %b_features_4_conv_1_1_running_mean, %b_features_4_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.1', 'features.4.conv.1.1', '_native_batch_norm_legit_no_training_10']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_30
min_val_cast
max_val_cast
hardtanh_7n4_8"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_7: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_7 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_30, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.1', 'features.4.conv.1.2', 'hardtanh_7']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ä

hardtanh_7
features.4.conv.2.weight
features.4.conv.2.weight_bias
getitem_33node_Conv_737"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_11: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_11 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_11, %p_features_4_conv_3_weight, %p_features_4_conv_3_bias, %b_features_4_conv_3_running_mean, %b_features_4_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.3', '_native_batch_norm_legit_no_training_11']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_33
features.5.conv.0.0.weight
features.5.conv.0.0.weight_bias
getitem_36node_Conv_739"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_12: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_12 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_12, %p_features_5_conv_0_1_weight, %p_features_5_conv_0_1_bias, %b_features_5_conv_0_1_running_mean, %b_features_5_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.0', 'features.5.conv.0.1', '_native_batch_norm_legit_no_training_12']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_36
min_val_cast
max_val_cast
hardtanh_8n4_9"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_8: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_8 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_36, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.0', 'features.5.conv.0.2', 'hardtanh_8']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
è

hardtanh_8
features.5.conv.1.0.weight
features.5.conv.1.0.weight_bias
getitem_39node_Conv_741"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_13: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_13 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_13, %p_features_5_conv_1_1_weight, %p_features_5_conv_1_1_bias, %b_features_5_conv_1_1_running_mean, %b_features_5_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.1', 'features.5.conv.1.1', '_native_batch_norm_legit_no_training_13']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_39
min_val_cast
max_val_cast
hardtanh_9n4_10"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_9: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_9 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_39, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.1', 'features.5.conv.1.2', 'hardtanh_9']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
é

hardtanh_9
features.5.conv.2.weight
features.5.conv.2.weight_bias
getitem_42node_Conv_743"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_14: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_14 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_14, %p_features_5_conv_3_weight, %p_features_5_conv_3_bias, %b_features_5_conv_3_running_mean, %b_features_5_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.3', '_native_batch_norm_legit_no_training_14']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_33

getitem_42add_6
node_add_6"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/add_6: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jí
pkg.torch.onnx.fx_nodex%add_6 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_33, %getitem_42), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.5', 'add_6']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
â
add_6
features.6.conv.0.0.weight
features.6.conv.0.0.weight_bias
getitem_45node_Conv_745"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_15: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_15 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_15, %p_features_6_conv_0_1_weight, %p_features_6_conv_0_1_bias, %b_features_6_conv_0_1_running_mean, %b_features_6_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.0', 'features.6.conv.0.1', '_native_batch_norm_legit_no_training_15']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_45
min_val_cast
max_val_casthardtanh_10n4_11"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_10: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_10 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_45, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.0', 'features.6.conv.0.2', 'hardtanh_10']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_10
features.6.conv.1.0.weight
features.6.conv.1.0.weight_bias
getitem_48node_Conv_747"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_16: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_16 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_16, %p_features_6_conv_1_1_weight, %p_features_6_conv_1_1_bias, %b_features_6_conv_1_1_running_mean, %b_features_6_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.1', 'features.6.conv.1.1', '_native_batch_norm_legit_no_training_16']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_48
min_val_cast
max_val_casthardtanh_11n4_12"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_11: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_11 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_48, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.1', 'features.6.conv.1.2', 'hardtanh_11']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
è
hardtanh_11
features.6.conv.2.weight
features.6.conv.2.weight_bias
getitem_51node_Conv_749"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_17: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_17 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_17, %p_features_6_conv_3_weight, %p_features_6_conv_3_bias, %b_features_6_conv_3_running_mean, %b_features_6_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.3', '_native_batch_norm_legit_no_training_17']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Ñ
add_6

getitem_51add_7
node_add_7"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/add_7: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_6, %getitem_51), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.6', 'add_7']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
Ö
add_7
features.7.conv.0.0.weight
features.7.conv.0.0.weight_bias
getitem_54node_Conv_751"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_18: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_18 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_18, %p_features_7_conv_0_1_weight, %p_features_7_conv_0_1_bias, %b_features_7_conv_0_1_running_mean, %b_features_7_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.0', 'features.7.conv.0.1', '_native_batch_norm_legit_no_training_18']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_54
min_val_cast
max_val_casthardtanh_12n4_13"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_12: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_12 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_54, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.0', 'features.7.conv.0.2', 'hardtanh_12']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
å
hardtanh_12
features.7.conv.1.0.weight
features.7.conv.1.0.weight_bias
getitem_57node_Conv_753"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_19: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_19 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_19, %p_features_7_conv_1_1_weight, %p_features_7_conv_1_1_bias, %b_features_7_conv_1_1_running_mean, %b_features_7_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.1', 'features.7.conv.1.1', '_native_batch_norm_legit_no_training_19']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_57
min_val_cast
max_val_casthardtanh_13n4_14"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_13: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_13 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_57, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.1', 'features.7.conv.1.2', 'hardtanh_13']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ã
hardtanh_13
features.7.conv.2.weight
features.7.conv.2.weight_bias
getitem_60node_Conv_755"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_20: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_20 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_20, %p_features_7_conv_3_weight, %p_features_7_conv_3_bias, %b_features_7_conv_3_running_mean, %b_features_7_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.3', '_native_batch_norm_legit_no_training_20']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_60
features.8.conv.0.0.weight
features.8.conv.0.0.weight_bias
getitem_63node_Conv_757"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_21: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_21 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_21, %p_features_8_conv_0_1_weight, %p_features_8_conv_0_1_bias, %b_features_8_conv_0_1_running_mean, %b_features_8_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.0', 'features.8.conv.0.1', '_native_batch_norm_legit_no_training_21']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_63
min_val_cast
max_val_casthardtanh_14n4_15"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_14: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_14 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_63, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.0', 'features.8.conv.0.2', 'hardtanh_14']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_14
features.8.conv.1.0.weight
features.8.conv.1.0.weight_bias
getitem_66node_Conv_759"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_22: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_22 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_22, %p_features_8_conv_1_1_weight, %p_features_8_conv_1_1_bias, %b_features_8_conv_1_1_running_mean, %b_features_8_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.1', 'features.8.conv.1.1', '_native_batch_norm_legit_no_training_22']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_66
min_val_cast
max_val_casthardtanh_15n4_16"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_15: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_15 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_66, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.1', 'features.8.conv.1.2', 'hardtanh_15']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
è
hardtanh_15
features.8.conv.2.weight
features.8.conv.2.weight_bias
getitem_69node_Conv_761"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_23: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_23 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_23, %p_features_8_conv_3_weight, %p_features_8_conv_3_bias, %b_features_8_conv_3_running_mean, %b_features_8_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.3', '_native_batch_norm_legit_no_training_23']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_60

getitem_69add_8
node_add_8"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/add_8: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jí
pkg.torch.onnx.fx_nodex%add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_60, %getitem_69), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.8', 'add_8']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
â
add_8
features.9.conv.0.0.weight
features.9.conv.0.0.weight_bias
getitem_72node_Conv_763"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_24: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_24 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_24, %p_features_9_conv_0_1_weight, %p_features_9_conv_0_1_bias, %b_features_9_conv_0_1_running_mean, %b_features_9_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.0', 'features.9.conv.0.1', '_native_batch_norm_legit_no_training_24']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_72
min_val_cast
max_val_casthardtanh_16n4_17"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_16: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_16 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_72, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.0', 'features.9.conv.0.2', 'hardtanh_16']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_16
features.9.conv.1.0.weight
features.9.conv.1.0.weight_bias
getitem_75node_Conv_765"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_25: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_25 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_25, %p_features_9_conv_1_1_weight, %p_features_9_conv_1_1_bias, %b_features_9_conv_1_1_running_mean, %b_features_9_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.1', 'features.9.conv.1.1', '_native_batch_norm_legit_no_training_25']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_75
min_val_cast
max_val_casthardtanh_17n4_18"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_17: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_17 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_75, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.1', 'features.9.conv.1.2', 'hardtanh_17']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
è
hardtanh_17
features.9.conv.2.weight
features.9.conv.2.weight_bias
getitem_78node_Conv_767"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_26: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_26 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_26, %p_features_9_conv_3_weight, %p_features_9_conv_3_bias, %b_features_9_conv_3_running_mean, %b_features_9_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.3', '_native_batch_norm_legit_no_training_26']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Ñ
add_8

getitem_78add_9
node_add_9"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/add_9: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_9 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %getitem_78), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.9', 'add_9']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ó
add_9
features.10.conv.0.0.weight
 features.10.conv.0.0.weight_bias
getitem_81node_Conv_769"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_27: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_27 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_27, %p_features_10_conv_0_1_weight, %p_features_10_conv_0_1_bias, %b_features_10_conv_0_1_running_mean, %b_features_10_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.0', 'features.10.conv.0.1', '_native_batch_norm_legit_no_training_27']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_81
min_val_cast
max_val_casthardtanh_18n4_19"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_18: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_18 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_81, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.0', 'features.10.conv.0.2', 'hardtanh_18']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
û
hardtanh_18
features.10.conv.1.0.weight
 features.10.conv.1.0.weight_bias
getitem_84node_Conv_771"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_28: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_28 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_28, %p_features_10_conv_1_1_weight, %p_features_10_conv_1_1_bias, %b_features_10_conv_1_1_running_mean, %b_features_10_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.1', 'features.10.conv.1.1', '_native_batch_norm_legit_no_training_28']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_84
min_val_cast
max_val_casthardtanh_19n4_20"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_19: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_19 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_84, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.1', 'features.10.conv.1.2', 'hardtanh_19']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
õ
hardtanh_19
features.10.conv.2.weight
features.10.conv.2.weight_bias
getitem_87node_Conv_773"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_29: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_29 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_29, %p_features_10_conv_3_weight, %p_features_10_conv_3_bias, %b_features_10_conv_3_running_mean, %b_features_10_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.3', '_native_batch_norm_legit_no_training_29']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ã
add_9

getitem_87add_10node_add_10"AddJø
	namespace±: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/add_10: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_9, %getitem_87), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'features', 'features.10', 'add_10']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
î
add_10
features.11.conv.0.0.weight
 features.11.conv.0.0.weight_bias
getitem_90node_Conv_775"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_30: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_30 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_30, %p_features_11_conv_0_1_weight, %p_features_11_conv_0_1_bias, %b_features_11_conv_0_1_running_mean, %b_features_11_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.0', 'features.11.conv.0.1', '_native_batch_norm_legit_no_training_30']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
™

getitem_90
min_val_cast
max_val_casthardtanh_20n4_21"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_20: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_20 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_90, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.0', 'features.11.conv.0.2', 'hardtanh_20']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ö
hardtanh_20
features.11.conv.1.0.weight
 features.11.conv.1.0.weight_bias
getitem_93node_Conv_777"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_31: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_31 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_31, %p_features_11_conv_1_1_weight, %p_features_11_conv_1_1_bias, %b_features_11_conv_1_1_running_mean, %b_features_11_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.1', 'features.11.conv.1.1', '_native_batch_norm_legit_no_training_31']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
™

getitem_93
min_val_cast
max_val_casthardtanh_21n4_22"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_21: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_21 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_93, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.1', 'features.11.conv.1.2', 'hardtanh_21']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ó
hardtanh_21
features.11.conv.2.weight
features.11.conv.2.weight_bias
getitem_96node_Conv_779"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_32: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_32 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_32, %p_features_11_conv_3_weight, %p_features_11_conv_3_bias, %b_features_11_conv_3_running_mean, %b_features_11_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.3', '_native_batch_norm_legit_no_training_32']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ú

getitem_96
features.12.conv.0.0.weight
 features.12.conv.0.0.weight_bias
getitem_99node_Conv_781"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_33: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_33 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_33, %p_features_12_conv_0_1_weight, %p_features_12_conv_0_1_bias, %b_features_12_conv_0_1_running_mean, %b_features_12_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.0', 'features.12.conv.0.1', '_native_batch_norm_legit_no_training_33']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_99
min_val_cast
max_val_casthardtanh_22n4_23"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_22: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_22 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_99, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.0', 'features.12.conv.0.2', 'hardtanh_22']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_22
features.12.conv.1.0.weight
 features.12.conv.1.0.weight_biasgetitem_102node_Conv_783"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_34: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_34 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_34, %p_features_12_conv_1_1_weight, %p_features_12_conv_1_1_bias, %b_features_12_conv_1_1_running_mean, %b_features_12_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.1', 'features.12.conv.1.1', '_native_batch_norm_legit_no_training_34']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_102
min_val_cast
max_val_casthardtanh_23n4_24"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_23: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_23 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_102, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.1', 'features.12.conv.1.2', 'hardtanh_23']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ú
hardtanh_23
features.12.conv.2.weight
features.12.conv.2.weight_biasgetitem_105node_Conv_785"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_35: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_35 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_35, %p_features_12_conv_3_weight, %p_features_12_conv_3_bias, %b_features_12_conv_3_running_mean, %b_features_12_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.3', '_native_batch_norm_legit_no_training_35']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ó

getitem_96
getitem_105add_11node_add_11"AddJø
	namespace±: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/add_11: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jî
pkg.torch.onnx.fx_nodez%add_11 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_96, %getitem_105), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'features', 'features.12', 'add_11']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ô
add_11
features.13.conv.0.0.weight
 features.13.conv.0.0.weight_biasgetitem_108node_Conv_787"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_36: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_36 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_36, %p_features_13_conv_0_1_weight, %p_features_13_conv_0_1_bias, %b_features_13_conv_0_1_running_mean, %b_features_13_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.0', 'features.13.conv.0.1', '_native_batch_norm_legit_no_training_36']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_108
min_val_cast
max_val_casthardtanh_24n4_25"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_24: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_24 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_108, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.0', 'features.13.conv.0.2', 'hardtanh_24']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_24
features.13.conv.1.0.weight
 features.13.conv.1.0.weight_biasgetitem_111node_Conv_789"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_37: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_37 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_37, %p_features_13_conv_1_1_weight, %p_features_13_conv_1_1_bias, %b_features_13_conv_1_1_running_mean, %b_features_13_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.1', 'features.13.conv.1.1', '_native_batch_norm_legit_no_training_37']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_111
min_val_cast
max_val_casthardtanh_25n4_26"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_25: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_25 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_111, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.1', 'features.13.conv.1.2', 'hardtanh_25']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ú
hardtanh_25
features.13.conv.2.weight
features.13.conv.2.weight_biasgetitem_114node_Conv_791"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_38: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_38 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_38, %p_features_13_conv_3_weight, %p_features_13_conv_3_bias, %b_features_13_conv_3_running_mean, %b_features_13_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.3', '_native_batch_norm_legit_no_training_38']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
è
add_11
getitem_114add_12node_add_12"AddJø
	namespace±: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/add_12: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jê
pkg.torch.onnx.fx_nodev%add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_11, %getitem_114), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'features', 'features.13', 'add_12']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ï
add_12
features.14.conv.0.0.weight
 features.14.conv.0.0.weight_biasgetitem_117node_Conv_793"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_39: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_39 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_39, %p_features_14_conv_0_1_weight, %p_features_14_conv_0_1_bias, %b_features_14_conv_0_1_running_mean, %b_features_14_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.0', 'features.14.conv.0.1', '_native_batch_norm_legit_no_training_39']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_117
min_val_cast
max_val_casthardtanh_26n4_27"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_26: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_26 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_117, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.0', 'features.14.conv.0.2', 'hardtanh_26']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
õ
hardtanh_26
features.14.conv.1.0.weight
 features.14.conv.1.0.weight_biasgetitem_120node_Conv_795"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_40: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_40 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_40, %p_features_14_conv_1_1_weight, %p_features_14_conv_1_1_bias, %b_features_14_conv_1_1_running_mean, %b_features_14_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.1', 'features.14.conv.1.1', '_native_batch_norm_legit_no_training_40']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_120
min_val_cast
max_val_casthardtanh_27n4_28"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_27: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_27 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_120, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.1', 'features.14.conv.1.2', 'hardtanh_27']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ò
hardtanh_27
features.14.conv.2.weight
features.14.conv.2.weight_biasgetitem_123node_Conv_797"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_41: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_41 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_41, %p_features_14_conv_3_weight, %p_features_14_conv_3_bias, %b_features_14_conv_3_running_mean, %b_features_14_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.3', '_native_batch_norm_legit_no_training_41']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
û
getitem_123
features.15.conv.0.0.weight
 features.15.conv.0.0.weight_biasgetitem_126node_Conv_799"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_42: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_42 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_42, %p_features_15_conv_0_1_weight, %p_features_15_conv_0_1_bias, %b_features_15_conv_0_1_running_mean, %b_features_15_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.0', 'features.15.conv.0.1', '_native_batch_norm_legit_no_training_42']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_126
min_val_cast
max_val_casthardtanh_28n4_29"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_28: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_28 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_126, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.0', 'features.15.conv.0.2', 'hardtanh_28']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_28
features.15.conv.1.0.weight
 features.15.conv.1.0.weight_biasgetitem_129node_Conv_801"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_43: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_43 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_43, %p_features_15_conv_1_1_weight, %p_features_15_conv_1_1_bias, %b_features_15_conv_1_1_running_mean, %b_features_15_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.1', 'features.15.conv.1.1', '_native_batch_norm_legit_no_training_43']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_129
min_val_cast
max_val_casthardtanh_29n4_30"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_29: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_29 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_129, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.1', 'features.15.conv.1.2', 'hardtanh_29']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ú
hardtanh_29
features.15.conv.2.weight
features.15.conv.2.weight_biasgetitem_132node_Conv_803"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_44: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_44 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_44, %p_features_15_conv_3_weight, %p_features_15_conv_3_bias, %b_features_15_conv_3_running_mean, %b_features_15_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.3', '_native_batch_norm_legit_no_training_44']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ô
getitem_123
getitem_132add_13node_add_13"AddJø
	namespace±: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/add_13: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jï
pkg.torch.onnx.fx_node{%add_13 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_123, %getitem_132), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'features', 'features.15', 'add_13']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ô
add_13
features.16.conv.0.0.weight
 features.16.conv.0.0.weight_biasgetitem_135node_Conv_805"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_45: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_45 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_45, %p_features_16_conv_0_1_weight, %p_features_16_conv_0_1_bias, %b_features_16_conv_0_1_running_mean, %b_features_16_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.0', 'features.16.conv.0.1', '_native_batch_norm_legit_no_training_45']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_135
min_val_cast
max_val_casthardtanh_30n4_31"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_30: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_30 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_135, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.0', 'features.16.conv.0.2', 'hardtanh_30']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_30
features.16.conv.1.0.weight
 features.16.conv.1.0.weight_biasgetitem_138node_Conv_807"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_46: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_46 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_46, %p_features_16_conv_1_1_weight, %p_features_16_conv_1_1_bias, %b_features_16_conv_1_1_running_mean, %b_features_16_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.1', 'features.16.conv.1.1', '_native_batch_norm_legit_no_training_46']JÀ
pkg.torch.onnx.stack_trace¨File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_138
min_val_cast
max_val_casthardtanh_31n4_32"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_31: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_31 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_138, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.1', 'features.16.conv.1.2', 'hardtanh_31']J˙
pkg.torch.onnx.stack_trace€File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ú
hardtanh_31
features.16.conv.2.weight
features.16.conv.2.weight_biasgetitem_141node_Conv_809"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_47: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_47 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_47, %p_features_16_conv_3_weight, %p_features_16_conv_3_bias, %b_features_16_conv_3_running_mean, %b_features_16_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.3', '_native_batch_norm_legit_no_training_47']J€	
pkg.torch.onnx.stack_traceº	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
è
add_13
getitem_141add_14node_add_14"AddJø
	namespace±: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/add_14: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jê
pkg.torch.onnx.fx_nodev%add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_13, %getitem_141), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'features', 'features.16', 'add_14']J¸
pkg.torch.onnx.stack_trace›File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ï
add_14
features.17.conv.0.0.weight
 features.17.conv.0.0.weight_biasgetitem_144node_Conv_811"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_48: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_48 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_48, %p_features_17_conv_0_1_weight, %p_features_17_conv_0_1_bias, %b_features_17_conv_0_1_running_mean, %b_features_17_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.0', 'features.17.conv.0.1', '_native_batch_norm_legit_no_training_48']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_144
min_val_cast
max_val_casthardtanh_32n4_33"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_32: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_32 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_144, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.0', 'features.17.conv.0.2', 'hardtanh_32']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
õ
hardtanh_32
features.17.conv.1.0.weight
 features.17.conv.1.0.weight_biasgetitem_147node_Conv_813"Conv*
group¿†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_49: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_49 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_49, %p_features_17_conv_1_1_weight, %p_features_17_conv_1_1_bias, %b_features_17_conv_1_1_running_mean, %b_features_17_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.1', 'features.17.conv.1.1', '_native_batch_norm_legit_no_training_49']J«
pkg.torch.onnx.stack_trace®File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_147
min_val_cast
max_val_casthardtanh_33n4_34"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_33: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_33 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_147, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.1', 'features.17.conv.1.2', 'hardtanh_33']Jˆ
pkg.torch.onnx.stack_trace◊File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ò
hardtanh_33
features.17.conv.2.weight
features.17.conv.2.weight_biasgetitem_150node_Conv_815"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_50: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_50 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_50, %p_features_17_conv_3_weight, %p_features_17_conv_3_bias, %b_features_17_conv_3_running_mean, %b_features_17_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.3', '_native_batch_norm_legit_no_training_50']J◊	
pkg.torch.onnx.stack_trace∏	File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
˝
getitem_150
features.18.0.weight
features.18.0.weight_biasgetitem_153node_Conv_817"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J≤
	namespace§: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.18: torchvision.ops.misc.Conv2dNormActivation/features.18.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_51: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJÜ
pkg.torch.onnx.class_hierarchy„['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J¬
pkg.torch.onnx.fx_nodeß%_native_batch_norm_legit_no_training_51 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_51, %p_features_18_1_weight, %p_features_18_1_bias, %b_features_18_1_running_mean, %b_features_18_1_running_var, 0.1, 1e-05), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'features', 'features.18', 'features.18.1', '_native_batch_norm_legit_no_training_51']JÊ
pkg.torch.onnx.stack_trace«File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∫
getitem_153
min_val_cast
max_val_casthardtanh_34n4_35"ClipJı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.18: torchvision.ops.misc.Conv2dNormActivation/features.18.2: torch.nn.modules.activation.ReLU6/hardtanh_34: aten.hardtanh.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_34 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_153, 0.0, 6.0), kwargs = {})J]
pkg.torch.onnx.name_scopes?['', 'features', 'features.18', 'features.18.2', 'hardtanh_34']Jï
pkg.torch.onnx.stack_traceˆFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ô
hardtanh_34
val_473mean	node_mean"
ReduceMean*
keepdims†*
noop_with_empty_axes †JM
	namespace@: torchvision.models.mobilenetv2.MobileNetV2/mean: aten.mean.dimJa
pkg.torch.onnx.class_hierarchy?['torchvision.models.mobilenetv2.MobileNetV2', 'aten.mean.dim']Jì
pkg.torch.onnx.fx_nodey%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%hardtanh_34, [-1, -2], True), kwargs = {})J*
pkg.torch.onnx.name_scopes['', 'mean']Jó
pkg.torch.onnx.stack_trace¯File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
…
mean
val_477view	node_view"Reshape*
	allowzero†JQ
	namespaceD: torchvision.models.mobilenetv2.MobileNetV2/view: aten.view.defaultJe
pkg.torch.onnx.class_hierarchyC['torchvision.models.mobilenetv2.MobileNetV2', 'aten.view.default']Jã
pkg.torch.onnx.fx_nodeq%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mean, [1, 1280]), kwargs = {})J*
pkg.torch.onnx.name_scopes['', 'view']Jó
pkg.torch.onnx.stack_trace¯File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
â
view
classifier.1.weight
classifier.1.biaslinearnode_linear"Gemm*
transA †*
transB†*
alpha  Ä?†*
beta  Ä?†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.1: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJ≥
pkg.torch.onnx.class_hierarchyê['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J¥
pkg.torch.onnx.fx_nodeô%linear : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%clone, %p_classifier_1_weight, %p_classifier_1_bias), kwargs = {})JJ
pkg.torch.onnx.name_scopes,['', 'classifier', 'classifier.1', 'linear']Jç
pkg.torch.onnx.stack_traceÓFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
¸
linear
val_480linalg_vector_normnode_linalg_vector_norm"ReduceL2*
keepdims†*
noop_with_empty_axes †J∆
	namespace∏: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.2: __main__.NormalizeLayer/linalg_vector_norm: aten.linalg_vector_norm.defaultJ∏
pkg.torch.onnx.class_hierarchyï['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', '__main__.NormalizeLayer', 'aten.linalg_vector_norm.default']J≠
pkg.torch.onnx.fx_nodeí%linalg_vector_norm : [num_users=1] = call_function[target=torch.ops.aten.linalg_vector_norm.default](args = (%linear, 2, [1], True), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'classifier', 'classifier.2', 'linalg_vector_norm']J˜
pkg.torch.onnx.stack_traceÿFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\tools\train_embedding.py", line 17, in forward
    return torch.nn.functional.normalize(x, p=2, dim=1)
ì
linalg_vector_norm
val_482	clamp_minnode_clamp_min"ClipJ¥
	namespace¶: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.2: __main__.NormalizeLayer/clamp_min: aten.clamp_min.defaultJØ
pkg.torch.onnx.class_hierarchyå['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', '__main__.NormalizeLayer', 'aten.clamp_min.default']J†
pkg.torch.onnx.fx_nodeÖ%clamp_min : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%linalg_vector_norm, 1e-12), kwargs = {})JM
pkg.torch.onnx.name_scopes/['', 'classifier', 'classifier.2', 'clamp_min']J˜
pkg.torch.onnx.stack_traceÿFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\tools\train_embedding.py", line 17, in forward
    return torch.nn.functional.normalize(x, p=2, dim=1)
Ì

	clamp_min
val_486expandnode_expand"ExpandJÆ
	namespace†: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.2: __main__.NormalizeLayer/expand: aten.expand.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', '__main__.NormalizeLayer', 'aten.expand.default']Jì
pkg.torch.onnx.fx_nodey%expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%clamp_min, [1, 128]), kwargs = {})JJ
pkg.torch.onnx.name_scopes,['', 'classifier', 'classifier.2', 'expand']J˜
pkg.torch.onnx.stack_traceÿFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\tools\train_embedding.py", line 17, in forward
    return torch.nn.functional.normalize(x, p=2, dim=1)
Õ

linear
expand	embeddingnode_div"DivJß
	namespaceô: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.2: __main__.NormalizeLayer/div: aten.div.TensorJ®
pkg.torch.onnx.class_hierarchyÖ['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', '__main__.NormalizeLayer', 'aten.div.Tensor']Jà
pkg.torch.onnx.fx_noden%div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%linear, %expand), kwargs = {})JG
pkg.torch.onnx.name_scopes)['', 'classifier', 'classifier.2', 'div']J˜
pkg.torch.onnx.stack_traceÿFile "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torchvision\models\mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\.venv\Lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "D:\SURYA\.DEVELOPMENT\.PROJECTS\Project No 390 Uma poly solutions Functional test jig\Raspberry Pi\LCD_QC_ML_V1.0\LCD_QC_Station\tools\train_embedding.py", line 17, in forward
    return torch.nn.functional.normalize(x, p=2, dim=1)
main_graph*f Bfeatures.0.0.weightj"
locationembedding_v2.onnx.dataj
offset41856j
length3456p*m Bfeatures.1.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset11264j
length1152p*k Bfeatures.1.conv.1.weightj"
locationembedding_v2.onnx.dataj
offset25984j
length2048p*m`Bfeatures.2.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset45312j
length3456p*]ÄBclassifier.1.biasj"
locationembedding_v2.onnx.dataj
offset1920j
length512p*m`Bfeatures.2.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset87296j
length6144p*l`Bfeatures.2.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset114176j
length9216p*pêBfeatures.3.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset123392j
length13824p*nêBfeatures.3.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset76928j
length5184p*nêBfeatures.3.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset137216j
length13824p*pêBfeatures.4.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset151040j
length13824p*nêBfeatures.4.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset82112j
length5184p*n êBfeatures.4.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset220160j
length18432p*p¿ Bfeatures.5.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset300800j
length24576p*n¿Bfeatures.5.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset93440j
length6912p*n ¿Bfeatures.5.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset325376j
length24576p*p¿ Bfeatures.6.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset349952j
length24576p*o¿Bfeatures.6.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset100352j
length6912p*n ¿Bfeatures.6.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset374528j
length24576p*p¿ Bfeatures.7.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset399104j
length24576p*o¿Bfeatures.7.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset107264j
length6912p*n@¿Bfeatures.7.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset527360j
length49152p*pÄ@Bfeatures.8.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset576512j
length98304p*pÄBfeatures.8.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset164864j
length13824p*n@ÄBfeatures.8.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset674816j
length98304p*pÄ@Bfeatures.9.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset773120j
length98304p*pÄBfeatures.9.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset178688j
length13824p*n@ÄBfeatures.9.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset871424j
length98304p*qÄ@Bfeatures.10.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset969728j
length98304p*qÄBfeatures.10.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset192512j
length13824p*p@ÄBfeatures.10.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset1068032j
length98304p*rÄ@Bfeatures.11.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset1166336j
length98304p*qÄBfeatures.11.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset206336j
length13824p*q`ÄBfeatures.11.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset1264640j
length147456p*s¿`Bfeatures.12.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset1412096j
length221184p*q¿Bfeatures.12.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset238592j
length20736p*q`¿Bfeatures.12.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset1633280j
length221184p*s¿`Bfeatures.13.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset1854464j
length221184p*q¿Bfeatures.13.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset259328j
length20736p*q`¿Bfeatures.13.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset2075648j
length221184p*s¿`Bfeatures.14.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset2296832j
length221184p*q¿Bfeatures.14.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset280064j
length20736p*r†¿Bfeatures.14.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset2518016j
length368640p*t¿†Bfeatures.15.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset2886656j
length614400p*q¿Bfeatures.15.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset423680j
length34560p*r†¿Bfeatures.15.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset3501056j
length614400p*t¿†Bfeatures.16.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset4115456j
length614400p*q¿Bfeatures.16.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset458240j
length34560p*r†¿Bfeatures.16.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset4729856j
length614400p*t¿†Bfeatures.17.conv.0.0.weightj"
locationembedding_v2.onnx.dataj
offset5344256j
length614400p*q¿Bfeatures.17.conv.1.0.weightj"
locationembedding_v2.onnx.dataj
offset492800j
length34560p*s¿¿Bfeatures.17.conv.2.weightj"
locationembedding_v2.onnx.dataj
offset6619136j
length1228800p*nÄ
¿Bfeatures.18.0.weightj"
locationembedding_v2.onnx.dataj
offset7864320j
length1638400p*hÄÄ
Bclassifier.1.weightj"
locationembedding_v2.onnx.dataj
offset5958656j
length655360p*Bmin_val_castJ    *Bmax_val_castJ  ¿@*Bval_473Jˇˇˇˇˇˇˇˇ˛ˇˇˇˇˇˇˇ*Bval_477J              *Bval_480J       *Bval_482JÃºå+*Bval_486J       Ä       *° Bfeatures.0.0.weight_biasJÄZ∞Ωw?ß≤>ﬂê>¨æw?'?z˝>ä¶?3#?¥@Ë8Â¨æïMu?{b‰>√ææﬁc∞πçÜœ;*Ω±lîºêóÆ> ÃÕ=¥záæ·¬˛>™ﬂ”>/f|º9ä˘æí≈? ‡öæ9®#?&Ñ˘Ω(éΩ;&…>kâä>*® Bfeatures.1.conv.0.0.weight_biasJÄËBºΩ€n? 8<·!÷>)ÆÖ>ÆmV?L£_?@É—>´–n@â˛òªí≥™>!!ß>˛˚N?”æ¥’πª&µªº|<Cˆ^<Ò <ç∞ü=ÕFìæ»?Ha?TÄ†:[nB?4fèæ#ªæä,a?‚x,?Ñ§>≠bx?.ŸOæ*eBfeatures.1.conv.1.weight_biasJ@
	¿@œ⁄>§ø.ÇÑ?ˇHœΩ	œÏæΩF)øâÕÂøêΩVø´‘øââŒ?jy⁄æ[8øÿ$/æUç—?Wtµ?*g`Bfeatures.2.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset0j
length384p*i`Bfeatures.2.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset384j
length384p*ÖBfeatures.2.conv.2.weight_biasJ`=ïøÀ2?Ùd3ø◊cEæEs Ω÷˛:øﬁiŸæÁ>>ÜHæÊ>ò;’¬¬<<–>Úˆ>‘µÊ?@ß?UR?∂HL>MìFæŒP¯æ9><ª∞>ıŸè>s">ﬁ*>*kêBfeatures.3.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset2432j
length576p*kêBfeatures.3.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset3008j
length576p*ÖBfeatures.3.conv.2.weight_biasJ`p´≥ø,Üø|§¥=3æ<æø"†øÑá?∞K?œ∑=Ïe…=‚AßΩ—¿°¨!øÕK?`jôøE¥>ÅÎØæ=ø>±Bø7‡ø‹V∞>¥‡p?‹¶X?R;¿ªj§?*kêBfeatures.4.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset3584j
length576p*kêBfeatures.4.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset4160j
length576p*¶ Bfeatures.4.conv.2.weight_biasJÄ—⁄Z?
N?´øà8?÷S>±¡øß¨Éæ!sHæ•=»¸◊ø&c<?©ÃÆ?Á ?O;^?πEs=§ø‘[.??CÕΩŒ7?™òæK…%æl—>ﬂæølΩ»>,©Eø¸ê?;—2æs›ëø?àé>: tøÆÚ#ø∞<1æ*k¿Bfeatures.5.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset6656j
length768p*k¿Bfeatures.5.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset7424j
length768p*¶ Bfeatures.5.conv.2.weight_biasJÄ–ê==m`>^9–æ°i,?£Ùuæ±—FΩ¯‡#>1Bææ[øı†≈=R}◊=í´IæÀ	◊æ…ªø<èî<<‹ΩæÇƒ>S<æh7>w¬Ò>ÇD?0•∂æI-b>∑D˜æÇ»=Û"æèπÁæGH?€ô?;(æõÏkæFì->*k¿Bfeatures.6.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset8192j
length768p*k¿Bfeatures.6.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset8960j
length768p*¶ Bfeatures.6.conv.2.weight_biasJÄ€Àæ±7j=ò“ÑæXwc>VûÜ>ÂNï>ìê∆æµ#\>ÅŒºá,?_l>Û¨>ƒJ->∑ªv>Ûå>÷øçø{ñøî•∂=5K?uU¢>œcJ>ãßëΩ¸<p>·Å≠Ωáäøõ£ææœT > ¢ùæ\J˝æÕ^¯>≈õßæ*k¿Bfeatures.7.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset9728j
length768p*l¿Bfeatures.7.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset10496j
length768p*¶@Bfeatures.7.conv.2.weight_biasJÄ6•æ‡¬æˇÃQøN‚åΩk∏økƒΩÉkÛ=Èø]øAÅ8ΩŸ$Ú?òv´ø#è?…˜Ö=˜˙æâ∏œ>0>Ó=_j[æ\Éõæ@êæ´∏ïæˇúEΩÆløbé6?óñﬂ>ˇnˇ=Í…æÁÅB?Ì◊AøÄB> µÉøYQ>xÂâ>_ªç?çîÏ>v5ò>{Í√ΩøÃΩÉìΩ†ÂëøTæÇËè?E3T?ç7ø√Uøü<ø}¶ßºeP>äë«æ$E:?kÖƒ>D„?ñõ=>Å¨ü?Ôœ?i+h?ˇmcº ≥s?dëø˘ó_>˝ÄêæLÇæ´Ö	?kgÅ?ç’3æ*mÄBfeatures.8.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset13696j
length1536p*mÄBfeatures.8.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset15232j
length1536p*¶@Bfeatures.8.conv.2.weight_biasJÄ–|ûæ/† æ+ π=Ñ'x>˚ æ<≤«<ø/øSÍõæc&˘<Éíâæh>™—≠=QSûΩß'à=tæÑ>œ‚ä>ymõ=F(ü>{áæÜ4æìåøzI<ÙÒ>`s>2 væCQx?Ã•!ææ0>@·=î"∆>6\.?^é‰ΩSõ>táíø(wÌæwÆ?yûLΩK>‹vãæR)ÕΩ6I©æ[àå>&ΩΩ?å?ïJ>òx*<ÆB?î=3>%Ç,>ÍøΩ'Ãë>∂◊>b‚^æ4·∞æ]Ÿ>≤+»>ÃﬂßΩøâ>jõI?ﬁ≈Q?Løñiπ@Ièæ*mÄBfeatures.9.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset16768j
length1536p*mÄBfeatures.9.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset18304j
length1536p*¶@Bfeatures.9.conv.2.weight_biasJÄˆwó>Á«>Ó(®æànæ5ıìæ`qêæ≈Ö>|∫û=wK‡æP•=1Ò%>gŒÜæUyÿΩímúæ{î=ÓÛï>èG#>HÊùæ:≥>˚›r=nDì>»MÓ>ô›è>¶<7ΩI[>˘B‘Ω§(>˜Í™º“€<ŸW/<YåK<âH§Ω“EÌº™Ê÷æâ">Xcæ⁄Eæ^´€æ$s˝=Ÿó´>LÛ.æ7tæö?>ﬁF>m≥->àK>E8ôΩ°¨>∂§dæHÉ<	ˆ>=]ß…æ°JôæÀ$£ΩGÙÒº{⁄ë<´√X<É≈N=>¶MæU–tæß{q?yø>Rπx>ø>æ*nÄB features.10.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset19840j
length1536p*nÄB features.10.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset21376j
length1536p*ß@Bfeatures.10.conv.2.weight_biasJÄ∑}ãæ´ê!æ⁄Ì>0ùø´o?ˇpæì·æ¸ö}>
;øp∑?ê8ÖΩ°æ£>ø=±ıæ7ŒæT¥Üæj c>«;*æ∏/è>…,+>‡Îù=ÃÖB>J6ø>⁄·‘º≤µΩ-£ñ> zÂ=\h(æ¶ziæ6¥˚ºÆøvõ1ø≥õj>∏Ûõ>X=c=âΩ«Læ/æ•/F>âíË>÷ñ<Z‘ı:∆=&™≤Ω°«æ‡≥|æ}˝∞æ?6j>B€£æ¥˚Ω§ÂºÆ¿≥æŸó>◊È<çŒëΩÈªK>Êﬂöæ$∑Qø‘9™æÔ`=O†æ¿€/Ω2>*nÄB features.11.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset22912j
length1536p*nÄB features.11.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset24448j
length1536p*h`Bfeatures.11.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset768j
length384p*n¿B features.12.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset28032j
length2304p*n¿B features.12.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset30336j
length2304p*i`Bfeatures.12.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset1152j
length384p*n¿B features.13.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset32640j
length2304p*n¿B features.13.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset34944j
length2304p*i`Bfeatures.13.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset1536j
length384p*n¿B features.14.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset37248j
length2304p*n¿B features.14.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset39552j
length2304p*j†Bfeatures.14.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset4736j
length640p*n¿B features.15.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset48768j
length3840p*n¿B features.15.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset52608j
length3840p*j†Bfeatures.15.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset5376j
length640p*n¿B features.16.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset56448j
length3840p*n¿B features.16.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset60288j
length3840p*j†Bfeatures.16.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset6016j
length640p*n¿B features.17.conv.0.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset64128j
length3840p*n¿B features.17.conv.1.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset67968j
length3840p*l¿Bfeatures.17.conv.2.weight_biasj"
locationembedding_v2.onnx.dataj
offset12416j
length1280p*gÄ
Bfeatures.18.0.weight_biasj"
locationembedding_v2.onnx.dataj
offset71808j
length5120pZ 
input

s77

Ä
Ä"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"&
!pkg.torch.onnx.original_node_namexbá
	embedding
	

Ä"?
0pkg.torch.export.graph_signature.OutputSpec.kindUSER_OUTPUT"(
!pkg.torch.onnx.original_node_namedivj-
features.0.0.weight

 


j4
features.1.conv.0.0.weight

 


j2
features.1.conv.1.weight


 

j4
features.2.conv.1.0.weight

`


j◊
classifier.1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"8
!pkg.torch.onnx.original_node_namep_classifier_1_biasj4
features.2.conv.0.0.weight

`


j2
features.2.conv.2.weight


`

j5
features.3.conv.0.0.weight

ê


j5
features.3.conv.1.0.weight

ê


j3
features.3.conv.2.weight


ê

j5
features.4.conv.0.0.weight

ê


j5
features.4.conv.1.0.weight

ê


j3
features.4.conv.2.weight

 
ê

j5
features.5.conv.0.0.weight

¿
 

j5
features.5.conv.1.0.weight

¿


j3
features.5.conv.2.weight

 
¿

j5
features.6.conv.0.0.weight

¿
 

j5
features.6.conv.1.0.weight

¿


j3
features.6.conv.2.weight

 
¿

j5
features.7.conv.0.0.weight

¿
 

j5
features.7.conv.1.0.weight

¿


j3
features.7.conv.2.weight

@
¿

j5
features.8.conv.0.0.weight

Ä
@

j5
features.8.conv.1.0.weight

Ä


j3
features.8.conv.2.weight

@
Ä

j5
features.9.conv.0.0.weight

Ä
@

j5
features.9.conv.1.0.weight

Ä


j3
features.9.conv.2.weight

@
Ä

j6
features.10.conv.0.0.weight

Ä
@

j6
features.10.conv.1.0.weight

Ä


j4
features.10.conv.2.weight

@
Ä

j6
features.11.conv.0.0.weight

Ä
@

j6
features.11.conv.1.0.weight

Ä


j4
features.11.conv.2.weight

`
Ä

j6
features.12.conv.0.0.weight

¿
`

j6
features.12.conv.1.0.weight

¿


j4
features.12.conv.2.weight

`
¿

j6
features.13.conv.0.0.weight

¿
`

j6
features.13.conv.1.0.weight

¿


j4
features.13.conv.2.weight

`
¿

j6
features.14.conv.0.0.weight

¿
`

j6
features.14.conv.1.0.weight

¿


j5
features.14.conv.2.weight

†
¿

j7
features.15.conv.0.0.weight

¿
†

j6
features.15.conv.1.0.weight

¿


j5
features.15.conv.2.weight

†
¿

j7
features.16.conv.0.0.weight

¿
†

j6
features.16.conv.1.0.weight

¿


j5
features.16.conv.2.weight

†
¿

j7
features.17.conv.0.0.weight

¿
†

j6
features.17.conv.1.0.weight

¿


j5
features.17.conv.2.weight

¿
¿

j0
features.18.0.weight

Ä

¿

j‡
classifier.1.weight


Ä
Ä
"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone":
!pkg.torch.onnx.original_node_namep_classifier_1_weightjK
min_val_cast
 "3
$pkg.onnxscript.optimizer.folded_from['min_val']jK
max_val_cast
 "3
$pkg.onnxscript.optimizer.folded_from['max_val']jU
val_473


">
$pkg.onnxscript.optimizer.folded_from['val_471', 'val_472']jU
val_477


">
$pkg.onnxscript.optimizer.folded_from['val_475', 'val_476']jU
val_480


">
$pkg.onnxscript.optimizer.folded_from['val_478', 'val_479']jF
val_482
 "3
$pkg.onnxscript.optimizer.folded_from['val_481']jU
val_486


">
$pkg.onnxscript.optimizer.folded_from['val_484', 'val_485']j&
features.0.0.weight_bias


 j-
features.1.conv.0.0.weight_bias


 j+
features.1.conv.1.weight_bias


j-
features.2.conv.0.0.weight_bias


`j-
features.2.conv.1.0.weight_bias


`j+
features.2.conv.2.weight_bias


j.
features.3.conv.0.0.weight_bias
	
êj.
features.3.conv.1.0.weight_bias
	
êj+
features.3.conv.2.weight_bias


j.
features.4.conv.0.0.weight_bias
	
êj.
features.4.conv.1.0.weight_bias
	
êj+
features.4.conv.2.weight_bias


 j.
features.5.conv.0.0.weight_bias
	
¿j.
features.5.conv.1.0.weight_bias
	
¿j+
features.5.conv.2.weight_bias


 j.
features.6.conv.0.0.weight_bias
	
¿j.
features.6.conv.1.0.weight_bias
	
¿j+
features.6.conv.2.weight_bias


 j.
features.7.conv.0.0.weight_bias
	
¿j.
features.7.conv.1.0.weight_bias
	
¿j+
features.7.conv.2.weight_bias


@j.
features.8.conv.0.0.weight_bias
	
Äj.
features.8.conv.1.0.weight_bias
	
Äj+
features.8.conv.2.weight_bias


@j.
features.9.conv.0.0.weight_bias
	
Äj.
features.9.conv.1.0.weight_bias
	
Äj+
features.9.conv.2.weight_bias


@j/
 features.10.conv.0.0.weight_bias
	
Äj/
 features.10.conv.1.0.weight_bias
	
Äj,
features.10.conv.2.weight_bias


@j/
 features.11.conv.0.0.weight_bias
	
Äj/
 features.11.conv.1.0.weight_bias
	
Äj,
features.11.conv.2.weight_bias


`j/
 features.12.conv.0.0.weight_bias
	
¿j/
 features.12.conv.1.0.weight_bias
	
¿j,
features.12.conv.2.weight_bias


`j/
 features.13.conv.0.0.weight_bias
	
¿j/
 features.13.conv.1.0.weight_bias
	
¿j,
features.13.conv.2.weight_bias


`j/
 features.14.conv.0.0.weight_bias
	
¿j/
 features.14.conv.1.0.weight_bias
	
¿j-
features.14.conv.2.weight_bias
	
†j/
 features.15.conv.0.0.weight_bias
	
¿j/
 features.15.conv.1.0.weight_bias
	
¿j-
features.15.conv.2.weight_bias
	
†j/
 features.16.conv.0.0.weight_bias
	
¿j/
 features.16.conv.1.0.weight_bias
	
¿j-
features.16.conv.2.weight_bias
	
†j/
 features.17.conv.0.0.weight_bias
	
¿j/
 features.17.conv.1.0.weight_bias
	
¿j-
features.17.conv.2.weight_bias
	
¿j(
features.18.0.weight_bias
	
Ä
j!
getitem


 
@
@j"
hardtanh


 
@
@j#
	getitem_3


 
@
@j$

hardtanh_1


 
@
@j#
	getitem_6



@
@j#
	getitem_9


`
@
@j$

hardtanh_2


`
@
@j$

getitem_12


`
 
 j$

hardtanh_3


`
 
 j$

getitem_15



 
 j%

getitem_18


ê
 
 j%

hardtanh_4


ê
 
 j%

getitem_21


ê
 
 j%

hardtanh_5


ê
 
 j$

getitem_24



 
 j
add_5



 
 j%

getitem_27


ê
 
 j%

hardtanh_6


ê
 
 j%

getitem_30


ê

j%

hardtanh_7


ê

j$

getitem_33


 

j%

getitem_36


¿

j%

hardtanh_8


¿

j%

getitem_39


¿

j%

hardtanh_9


¿

j$

getitem_42


 

j
add_6


 

j%

getitem_45


¿

j&
hardtanh_10


¿

j%

getitem_48


¿

j&
hardtanh_11


¿

j$

getitem_51


 

j
add_7


 

j%

getitem_54


¿

j&
hardtanh_12


¿

j%

getitem_57


¿

j&
hardtanh_13


¿

j$

getitem_60


@

j%

getitem_63


Ä

j&
hardtanh_14


Ä

j%

getitem_66


Ä

j&
hardtanh_15


Ä

j$

getitem_69


@

j
add_8


@

j%

getitem_72


Ä

j&
hardtanh_16


Ä

j%

getitem_75


Ä

j&
hardtanh_17


Ä

j$

getitem_78


@

j
add_9


@

j%

getitem_81


Ä

j&
hardtanh_18


Ä

j%

getitem_84


Ä

j&
hardtanh_19


Ä

j$

getitem_87


@

j 
add_10


@

j%

getitem_90


Ä

j&
hardtanh_20


Ä

j%

getitem_93


Ä

j&
hardtanh_21


Ä

j$

getitem_96


`

j%

getitem_99


¿

j&
hardtanh_22


¿

j&
getitem_102


¿

j&
hardtanh_23


¿

j%
getitem_105


`

j 
add_11


`

j&
getitem_108


¿

j&
hardtanh_24


¿

j&
getitem_111


¿

j&
hardtanh_25


¿

j%
getitem_114


`

j 
add_12


`

j&
getitem_117


¿

j&
hardtanh_26


¿

j&
getitem_120


¿

j&
hardtanh_27


¿

j&
getitem_123


†

j&
getitem_126


¿

j&
hardtanh_28


¿

j&
getitem_129


¿

j&
hardtanh_29


¿

j&
getitem_132


†

j!
add_13


†

j&
getitem_135


¿

j&
hardtanh_30


¿

j&
getitem_138


¿

j&
hardtanh_31


¿

j&
getitem_141


†

j!
add_14


†

j&
getitem_144


¿

j&
hardtanh_32


¿

j&
getitem_147


¿

j&
hardtanh_33


¿

j&
getitem_150


¿

j&
getitem_153


Ä


j&
hardtanh_34


Ä


j
mean


Ä


j
view
	

Ä
j
linear
	

Äj$
linalg_vector_norm


j
	clamp_min


j
expand
	

ÄÇ ⁄
0pkg.torch.export.ExportedProgram.graph_signatureî⁄
# inputs
p_features_0_0_weight: PARAMETER target='features.0.0.weight'
p_features_0_1_weight: PARAMETER target='features.0.1.weight'
p_features_0_1_bias: PARAMETER target='features.0.1.bias'
p_features_1_conv_0_0_weight: PARAMETER target='features.1.conv.0.0.weight'
p_features_1_conv_0_1_weight: PARAMETER target='features.1.conv.0.1.weight'
p_features_1_conv_0_1_bias: PARAMETER target='features.1.conv.0.1.bias'
p_features_1_conv_1_weight: PARAMETER target='features.1.conv.1.weight'
p_features_1_conv_2_weight: PARAMETER target='features.1.conv.2.weight'
p_features_1_conv_2_bias: PARAMETER target='features.1.conv.2.bias'
p_features_2_conv_0_0_weight: PARAMETER target='features.2.conv.0.0.weight'
p_features_2_conv_0_1_weight: PARAMETER target='features.2.conv.0.1.weight'
p_features_2_conv_0_1_bias: PARAMETER target='features.2.conv.0.1.bias'
p_features_2_conv_1_0_weight: PARAMETER target='features.2.conv.1.0.weight'
p_features_2_conv_1_1_weight: PARAMETER target='features.2.conv.1.1.weight'
p_features_2_conv_1_1_bias: PARAMETER target='features.2.conv.1.1.bias'
p_features_2_conv_2_weight: PARAMETER target='features.2.conv.2.weight'
p_features_2_conv_3_weight: PARAMETER target='features.2.conv.3.weight'
p_features_2_conv_3_bias: PARAMETER target='features.2.conv.3.bias'
p_features_3_conv_0_0_weight: PARAMETER target='features.3.conv.0.0.weight'
p_features_3_conv_0_1_weight: PARAMETER target='features.3.conv.0.1.weight'
p_features_3_conv_0_1_bias: PARAMETER target='features.3.conv.0.1.bias'
p_features_3_conv_1_0_weight: PARAMETER target='features.3.conv.1.0.weight'
p_features_3_conv_1_1_weight: PARAMETER target='features.3.conv.1.1.weight'
p_features_3_conv_1_1_bias: PARAMETER target='features.3.conv.1.1.bias'
p_features_3_conv_2_weight: PARAMETER target='features.3.conv.2.weight'
p_features_3_conv_3_weight: PARAMETER target='features.3.conv.3.weight'
p_features_3_conv_3_bias: PARAMETER target='features.3.conv.3.bias'
p_features_4_conv_0_0_weight: PARAMETER target='features.4.conv.0.0.weight'
p_features_4_conv_0_1_weight: PARAMETER target='features.4.conv.0.1.weight'
p_features_4_conv_0_1_bias: PARAMETER target='features.4.conv.0.1.bias'
p_features_4_conv_1_0_weight: PARAMETER target='features.4.conv.1.0.weight'
p_features_4_conv_1_1_weight: PARAMETER target='features.4.conv.1.1.weight'
p_features_4_conv_1_1_bias: PARAMETER target='features.4.conv.1.1.bias'
p_features_4_conv_2_weight: PARAMETER target='features.4.conv.2.weight'
p_features_4_conv_3_weight: PARAMETER target='features.4.conv.3.weight'
p_features_4_conv_3_bias: PARAMETER target='features.4.conv.3.bias'
p_features_5_conv_0_0_weight: PARAMETER target='features.5.conv.0.0.weight'
p_features_5_conv_0_1_weight: PARAMETER target='features.5.conv.0.1.weight'
p_features_5_conv_0_1_bias: PARAMETER target='features.5.conv.0.1.bias'
p_features_5_conv_1_0_weight: PARAMETER target='features.5.conv.1.0.weight'
p_features_5_conv_1_1_weight: PARAMETER target='features.5.conv.1.1.weight'
p_features_5_conv_1_1_bias: PARAMETER target='features.5.conv.1.1.bias'
p_features_5_conv_2_weight: PARAMETER target='features.5.conv.2.weight'
p_features_5_conv_3_weight: PARAMETER target='features.5.conv.3.weight'
p_features_5_conv_3_bias: PARAMETER target='features.5.conv.3.bias'
p_features_6_conv_0_0_weight: PARAMETER target='features.6.conv.0.0.weight'
p_features_6_conv_0_1_weight: PARAMETER target='features.6.conv.0.1.weight'
p_features_6_conv_0_1_bias: PARAMETER target='features.6.conv.0.1.bias'
p_features_6_conv_1_0_weight: PARAMETER target='features.6.conv.1.0.weight'
p_features_6_conv_1_1_weight: PARAMETER target='features.6.conv.1.1.weight'
p_features_6_conv_1_1_bias: PARAMETER target='features.6.conv.1.1.bias'
p_features_6_conv_2_weight: PARAMETER target='features.6.conv.2.weight'
p_features_6_conv_3_weight: PARAMETER target='features.6.conv.3.weight'
p_features_6_conv_3_bias: PARAMETER target='features.6.conv.3.bias'
p_features_7_conv_0_0_weight: PARAMETER target='features.7.conv.0.0.weight'
p_features_7_conv_0_1_weight: PARAMETER target='features.7.conv.0.1.weight'
p_features_7_conv_0_1_bias: PARAMETER target='features.7.conv.0.1.bias'
p_features_7_conv_1_0_weight: PARAMETER target='features.7.conv.1.0.weight'
p_features_7_conv_1_1_weight: PARAMETER target='features.7.conv.1.1.weight'
p_features_7_conv_1_1_bias: PARAMETER target='features.7.conv.1.1.bias'
p_features_7_conv_2_weight: PARAMETER target='features.7.conv.2.weight'
p_features_7_conv_3_weight: PARAMETER target='features.7.conv.3.weight'
p_features_7_conv_3_bias: PARAMETER target='features.7.conv.3.bias'
p_features_8_conv_0_0_weight: PARAMETER target='features.8.conv.0.0.weight'
p_features_8_conv_0_1_weight: PARAMETER target='features.8.conv.0.1.weight'
p_features_8_conv_0_1_bias: PARAMETER target='features.8.conv.0.1.bias'
p_features_8_conv_1_0_weight: PARAMETER target='features.8.conv.1.0.weight'
p_features_8_conv_1_1_weight: PARAMETER target='features.8.conv.1.1.weight'
p_features_8_conv_1_1_bias: PARAMETER target='features.8.conv.1.1.bias'
p_features_8_conv_2_weight: PARAMETER target='features.8.conv.2.weight'
p_features_8_conv_3_weight: PARAMETER target='features.8.conv.3.weight'
p_features_8_conv_3_bias: PARAMETER target='features.8.conv.3.bias'
p_features_9_conv_0_0_weight: PARAMETER target='features.9.conv.0.0.weight'
p_features_9_conv_0_1_weight: PARAMETER target='features.9.conv.0.1.weight'
p_features_9_conv_0_1_bias: PARAMETER target='features.9.conv.0.1.bias'
p_features_9_conv_1_0_weight: PARAMETER target='features.9.conv.1.0.weight'
p_features_9_conv_1_1_weight: PARAMETER target='features.9.conv.1.1.weight'
p_features_9_conv_1_1_bias: PARAMETER target='features.9.conv.1.1.bias'
p_features_9_conv_2_weight: PARAMETER target='features.9.conv.2.weight'
p_features_9_conv_3_weight: PARAMETER target='features.9.conv.3.weight'
p_features_9_conv_3_bias: PARAMETER target='features.9.conv.3.bias'
p_features_10_conv_0_0_weight: PARAMETER target='features.10.conv.0.0.weight'
p_features_10_conv_0_1_weight: PARAMETER target='features.10.conv.0.1.weight'
p_features_10_conv_0_1_bias: PARAMETER target='features.10.conv.0.1.bias'
p_features_10_conv_1_0_weight: PARAMETER target='features.10.conv.1.0.weight'
p_features_10_conv_1_1_weight: PARAMETER target='features.10.conv.1.1.weight'
p_features_10_conv_1_1_bias: PARAMETER target='features.10.conv.1.1.bias'
p_features_10_conv_2_weight: PARAMETER target='features.10.conv.2.weight'
p_features_10_conv_3_weight: PARAMETER target='features.10.conv.3.weight'
p_features_10_conv_3_bias: PARAMETER target='features.10.conv.3.bias'
p_features_11_conv_0_0_weight: PARAMETER target='features.11.conv.0.0.weight'
p_features_11_conv_0_1_weight: PARAMETER target='features.11.conv.0.1.weight'
p_features_11_conv_0_1_bias: PARAMETER target='features.11.conv.0.1.bias'
p_features_11_conv_1_0_weight: PARAMETER target='features.11.conv.1.0.weight'
p_features_11_conv_1_1_weight: PARAMETER target='features.11.conv.1.1.weight'
p_features_11_conv_1_1_bias: PARAMETER target='features.11.conv.1.1.bias'
p_features_11_conv_2_weight: PARAMETER target='features.11.conv.2.weight'
p_features_11_conv_3_weight: PARAMETER target='features.11.conv.3.weight'
p_features_11_conv_3_bias: PARAMETER target='features.11.conv.3.bias'
p_features_12_conv_0_0_weight: PARAMETER target='features.12.conv.0.0.weight'
p_features_12_conv_0_1_weight: PARAMETER target='features.12.conv.0.1.weight'
p_features_12_conv_0_1_bias: PARAMETER target='features.12.conv.0.1.bias'
p_features_12_conv_1_0_weight: PARAMETER target='features.12.conv.1.0.weight'
p_features_12_conv_1_1_weight: PARAMETER target='features.12.conv.1.1.weight'
p_features_12_conv_1_1_bias: PARAMETER target='features.12.conv.1.1.bias'
p_features_12_conv_2_weight: PARAMETER target='features.12.conv.2.weight'
p_features_12_conv_3_weight: PARAMETER target='features.12.conv.3.weight'
p_features_12_conv_3_bias: PARAMETER target='features.12.conv.3.bias'
p_features_13_conv_0_0_weight: PARAMETER target='features.13.conv.0.0.weight'
p_features_13_conv_0_1_weight: PARAMETER target='features.13.conv.0.1.weight'
p_features_13_conv_0_1_bias: PARAMETER target='features.13.conv.0.1.bias'
p_features_13_conv_1_0_weight: PARAMETER target='features.13.conv.1.0.weight'
p_features_13_conv_1_1_weight: PARAMETER target='features.13.conv.1.1.weight'
p_features_13_conv_1_1_bias: PARAMETER target='features.13.conv.1.1.bias'
p_features_13_conv_2_weight: PARAMETER target='features.13.conv.2.weight'
p_features_13_conv_3_weight: PARAMETER target='features.13.conv.3.weight'
p_features_13_conv_3_bias: PARAMETER target='features.13.conv.3.bias'
p_features_14_conv_0_0_weight: PARAMETER target='features.14.conv.0.0.weight'
p_features_14_conv_0_1_weight: PARAMETER target='features.14.conv.0.1.weight'
p_features_14_conv_0_1_bias: PARAMETER target='features.14.conv.0.1.bias'
p_features_14_conv_1_0_weight: PARAMETER target='features.14.conv.1.0.weight'
p_features_14_conv_1_1_weight: PARAMETER target='features.14.conv.1.1.weight'
p_features_14_conv_1_1_bias: PARAMETER target='features.14.conv.1.1.bias'
p_features_14_conv_2_weight: PARAMETER target='features.14.conv.2.weight'
p_features_14_conv_3_weight: PARAMETER target='features.14.conv.3.weight'
p_features_14_conv_3_bias: PARAMETER target='features.14.conv.3.bias'
p_features_15_conv_0_0_weight: PARAMETER target='features.15.conv.0.0.weight'
p_features_15_conv_0_1_weight: PARAMETER target='features.15.conv.0.1.weight'
p_features_15_conv_0_1_bias: PARAMETER target='features.15.conv.0.1.bias'
p_features_15_conv_1_0_weight: PARAMETER target='features.15.conv.1.0.weight'
p_features_15_conv_1_1_weight: PARAMETER target='features.15.conv.1.1.weight'
p_features_15_conv_1_1_bias: PARAMETER target='features.15.conv.1.1.bias'
p_features_15_conv_2_weight: PARAMETER target='features.15.conv.2.weight'
p_features_15_conv_3_weight: PARAMETER target='features.15.conv.3.weight'
p_features_15_conv_3_bias: PARAMETER target='features.15.conv.3.bias'
p_features_16_conv_0_0_weight: PARAMETER target='features.16.conv.0.0.weight'
p_features_16_conv_0_1_weight: PARAMETER target='features.16.conv.0.1.weight'
p_features_16_conv_0_1_bias: PARAMETER target='features.16.conv.0.1.bias'
p_features_16_conv_1_0_weight: PARAMETER target='features.16.conv.1.0.weight'
p_features_16_conv_1_1_weight: PARAMETER target='features.16.conv.1.1.weight'
p_features_16_conv_1_1_bias: PARAMETER target='features.16.conv.1.1.bias'
p_features_16_conv_2_weight: PARAMETER target='features.16.conv.2.weight'
p_features_16_conv_3_weight: PARAMETER target='features.16.conv.3.weight'
p_features_16_conv_3_bias: PARAMETER target='features.16.conv.3.bias'
p_features_17_conv_0_0_weight: PARAMETER target='features.17.conv.0.0.weight'
p_features_17_conv_0_1_weight: PARAMETER target='features.17.conv.0.1.weight'
p_features_17_conv_0_1_bias: PARAMETER target='features.17.conv.0.1.bias'
p_features_17_conv_1_0_weight: PARAMETER target='features.17.conv.1.0.weight'
p_features_17_conv_1_1_weight: PARAMETER target='features.17.conv.1.1.weight'
p_features_17_conv_1_1_bias: PARAMETER target='features.17.conv.1.1.bias'
p_features_17_conv_2_weight: PARAMETER target='features.17.conv.2.weight'
p_features_17_conv_3_weight: PARAMETER target='features.17.conv.3.weight'
p_features_17_conv_3_bias: PARAMETER target='features.17.conv.3.bias'
p_features_18_0_weight: PARAMETER target='features.18.0.weight'
p_features_18_1_weight: PARAMETER target='features.18.1.weight'
p_features_18_1_bias: PARAMETER target='features.18.1.bias'
p_classifier_1_weight: PARAMETER target='classifier.1.weight'
p_classifier_1_bias: PARAMETER target='classifier.1.bias'
b_features_0_1_running_mean: BUFFER target='features.0.1.running_mean' persistent=True
b_features_0_1_running_var: BUFFER target='features.0.1.running_var' persistent=True
b_features_0_1_num_batches_tracked: BUFFER target='features.0.1.num_batches_tracked' persistent=True
b_features_1_conv_0_1_running_mean: BUFFER target='features.1.conv.0.1.running_mean' persistent=True
b_features_1_conv_0_1_running_var: BUFFER target='features.1.conv.0.1.running_var' persistent=True
b_features_1_conv_0_1_num_batches_tracked: BUFFER target='features.1.conv.0.1.num_batches_tracked' persistent=True
b_features_1_conv_2_running_mean: BUFFER target='features.1.conv.2.running_mean' persistent=True
b_features_1_conv_2_running_var: BUFFER target='features.1.conv.2.running_var' persistent=True
b_features_1_conv_2_num_batches_tracked: BUFFER target='features.1.conv.2.num_batches_tracked' persistent=True
b_features_2_conv_0_1_running_mean: BUFFER target='features.2.conv.0.1.running_mean' persistent=True
b_features_2_conv_0_1_running_var: BUFFER target='features.2.conv.0.1.running_var' persistent=True
b_features_2_conv_0_1_num_batches_tracked: BUFFER target='features.2.conv.0.1.num_batches_tracked' persistent=True
b_features_2_conv_1_1_running_mean: BUFFER target='features.2.conv.1.1.running_mean' persistent=True
b_features_2_conv_1_1_running_var: BUFFER target='features.2.conv.1.1.running_var' persistent=True
b_features_2_conv_1_1_num_batches_tracked: BUFFER target='features.2.conv.1.1.num_batches_tracked' persistent=True
b_features_2_conv_3_running_mean: BUFFER target='features.2.conv.3.running_mean' persistent=True
b_features_2_conv_3_running_var: BUFFER target='features.2.conv.3.running_var' persistent=True
b_features_2_conv_3_num_batches_tracked: BUFFER target='features.2.conv.3.num_batches_tracked' persistent=True
b_features_3_conv_0_1_running_mean: BUFFER target='features.3.conv.0.1.running_mean' persistent=True
b_features_3_conv_0_1_running_var: BUFFER target='features.3.conv.0.1.running_var' persistent=True
b_features_3_conv_0_1_num_batches_tracked: BUFFER target='features.3.conv.0.1.num_batches_tracked' persistent=True
b_features_3_conv_1_1_running_mean: BUFFER target='features.3.conv.1.1.running_mean' persistent=True
b_features_3_conv_1_1_running_var: BUFFER target='features.3.conv.1.1.running_var' persistent=True
b_features_3_conv_1_1_num_batches_tracked: BUFFER target='features.3.conv.1.1.num_batches_tracked' persistent=True
b_features_3_conv_3_running_mean: BUFFER target='features.3.conv.3.running_mean' persistent=True
b_features_3_conv_3_running_var: BUFFER target='features.3.conv.3.running_var' persistent=True
b_features_3_conv_3_num_batches_tracked: BUFFER target='features.3.conv.3.num_batches_tracked' persistent=True
b_features_4_conv_0_1_running_mean: BUFFER target='features.4.conv.0.1.running_mean' persistent=True
b_features_4_conv_0_1_running_var: BUFFER target='features.4.conv.0.1.running_var' persistent=True
b_features_4_conv_0_1_num_batches_tracked: BUFFER target='features.4.conv.0.1.num_batches_tracked' persistent=True
b_features_4_conv_1_1_running_mean: BUFFER target='features.4.conv.1.1.running_mean' persistent=True
b_features_4_conv_1_1_running_var: BUFFER target='features.4.conv.1.1.running_var' persistent=True
b_features_4_conv_1_1_num_batches_tracked: BUFFER target='features.4.conv.1.1.num_batches_tracked' persistent=True
b_features_4_conv_3_running_mean: BUFFER target='features.4.conv.3.running_mean' persistent=True
b_features_4_conv_3_running_var: BUFFER target='features.4.conv.3.running_var' persistent=True
b_features_4_conv_3_num_batches_tracked: BUFFER target='features.4.conv.3.num_batches_tracked' persistent=True
b_features_5_conv_0_1_running_mean: BUFFER target='features.5.conv.0.1.running_mean' persistent=True
b_features_5_conv_0_1_running_var: BUFFER target='features.5.conv.0.1.running_var' persistent=True
b_features_5_conv_0_1_num_batches_tracked: BUFFER target='features.5.conv.0.1.num_batches_tracked' persistent=True
b_features_5_conv_1_1_running_mean: BUFFER target='features.5.conv.1.1.running_mean' persistent=True
b_features_5_conv_1_1_running_var: BUFFER target='features.5.conv.1.1.running_var' persistent=True
b_features_5_conv_1_1_num_batches_tracked: BUFFER target='features.5.conv.1.1.num_batches_tracked' persistent=True
b_features_5_conv_3_running_mean: BUFFER target='features.5.conv.3.running_mean' persistent=True
b_features_5_conv_3_running_var: BUFFER target='features.5.conv.3.running_var' persistent=True
b_features_5_conv_3_num_batches_tracked: BUFFER target='features.5.conv.3.num_batches_tracked' persistent=True
b_features_6_conv_0_1_running_mean: BUFFER target='features.6.conv.0.1.running_mean' persistent=True
b_features_6_conv_0_1_running_var: BUFFER target='features.6.conv.0.1.running_var' persistent=True
b_features_6_conv_0_1_num_batches_tracked: BUFFER target='features.6.conv.0.1.num_batches_tracked' persistent=True
b_features_6_conv_1_1_running_mean: BUFFER target='features.6.conv.1.1.running_mean' persistent=True
b_features_6_conv_1_1_running_var: BUFFER target='features.6.conv.1.1.running_var' persistent=True
b_features_6_conv_1_1_num_batches_tracked: BUFFER target='features.6.conv.1.1.num_batches_tracked' persistent=True
b_features_6_conv_3_running_mean: BUFFER target='features.6.conv.3.running_mean' persistent=True
b_features_6_conv_3_running_var: BUFFER target='features.6.conv.3.running_var' persistent=True
b_features_6_conv_3_num_batches_tracked: BUFFER target='features.6.conv.3.num_batches_tracked' persistent=True
b_features_7_conv_0_1_running_mean: BUFFER target='features.7.conv.0.1.running_mean' persistent=True
b_features_7_conv_0_1_running_var: BUFFER target='features.7.conv.0.1.running_var' persistent=True
b_features_7_conv_0_1_num_batches_tracked: BUFFER target='features.7.conv.0.1.num_batches_tracked' persistent=True
b_features_7_conv_1_1_running_mean: BUFFER target='features.7.conv.1.1.running_mean' persistent=True
b_features_7_conv_1_1_running_var: BUFFER target='features.7.conv.1.1.running_var' persistent=True
b_features_7_conv_1_1_num_batches_tracked: BUFFER target='features.7.conv.1.1.num_batches_tracked' persistent=True
b_features_7_conv_3_running_mean: BUFFER target='features.7.conv.3.running_mean' persistent=True
b_features_7_conv_3_running_var: BUFFER target='features.7.conv.3.running_var' persistent=True
b_features_7_conv_3_num_batches_tracked: BUFFER target='features.7.conv.3.num_batches_tracked' persistent=True
b_features_8_conv_0_1_running_mean: BUFFER target='features.8.conv.0.1.running_mean' persistent=True
b_features_8_conv_0_1_running_var: BUFFER target='features.8.conv.0.1.running_var' persistent=True
b_features_8_conv_0_1_num_batches_tracked: BUFFER target='features.8.conv.0.1.num_batches_tracked' persistent=True
b_features_8_conv_1_1_running_mean: BUFFER target='features.8.conv.1.1.running_mean' persistent=True
b_features_8_conv_1_1_running_var: BUFFER target='features.8.conv.1.1.running_var' persistent=True
b_features_8_conv_1_1_num_batches_tracked: BUFFER target='features.8.conv.1.1.num_batches_tracked' persistent=True
b_features_8_conv_3_running_mean: BUFFER target='features.8.conv.3.running_mean' persistent=True
b_features_8_conv_3_running_var: BUFFER target='features.8.conv.3.running_var' persistent=True
b_features_8_conv_3_num_batches_tracked: BUFFER target='features.8.conv.3.num_batches_tracked' persistent=True
b_features_9_conv_0_1_running_mean: BUFFER target='features.9.conv.0.1.running_mean' persistent=True
b_features_9_conv_0_1_running_var: BUFFER target='features.9.conv.0.1.running_var' persistent=True
b_features_9_conv_0_1_num_batches_tracked: BUFFER target='features.9.conv.0.1.num_batches_tracked' persistent=True
b_features_9_conv_1_1_running_mean: BUFFER target='features.9.conv.1.1.running_mean' persistent=True
b_features_9_conv_1_1_running_var: BUFFER target='features.9.conv.1.1.running_var' persistent=True
b_features_9_conv_1_1_num_batches_tracked: BUFFER target='features.9.conv.1.1.num_batches_tracked' persistent=True
b_features_9_conv_3_running_mean: BUFFER target='features.9.conv.3.running_mean' persistent=True
b_features_9_conv_3_running_var: BUFFER target='features.9.conv.3.running_var' persistent=True
b_features_9_conv_3_num_batches_tracked: BUFFER target='features.9.conv.3.num_batches_tracked' persistent=True
b_features_10_conv_0_1_running_mean: BUFFER target='features.10.conv.0.1.running_mean' persistent=True
b_features_10_conv_0_1_running_var: BUFFER target='features.10.conv.0.1.running_var' persistent=True
b_features_10_conv_0_1_num_batches_tracked: BUFFER target='features.10.conv.0.1.num_batches_tracked' persistent=True
b_features_10_conv_1_1_running_mean: BUFFER target='features.10.conv.1.1.running_mean' persistent=True
b_features_10_conv_1_1_running_var: BUFFER target='features.10.conv.1.1.running_var' persistent=True
b_features_10_conv_1_1_num_batches_tracked: BUFFER target='features.10.conv.1.1.num_batches_tracked' persistent=True
b_features_10_conv_3_running_mean: BUFFER target='features.10.conv.3.running_mean' persistent=True
b_features_10_conv_3_running_var: BUFFER target='features.10.conv.3.running_var' persistent=True
b_features_10_conv_3_num_batches_tracked: BUFFER target='features.10.conv.3.num_batches_tracked' persistent=True
b_features_11_conv_0_1_running_mean: BUFFER target='features.11.conv.0.1.running_mean' persistent=True
b_features_11_conv_0_1_running_var: BUFFER target='features.11.conv.0.1.running_var' persistent=True
b_features_11_conv_0_1_num_batches_tracked: BUFFER target='features.11.conv.0.1.num_batches_tracked' persistent=True
b_features_11_conv_1_1_running_mean: BUFFER target='features.11.conv.1.1.running_mean' persistent=True
b_features_11_conv_1_1_running_var: BUFFER target='features.11.conv.1.1.running_var' persistent=True
b_features_11_conv_1_1_num_batches_tracked: BUFFER target='features.11.conv.1.1.num_batches_tracked' persistent=True
b_features_11_conv_3_running_mean: BUFFER target='features.11.conv.3.running_mean' persistent=True
b_features_11_conv_3_running_var: BUFFER target='features.11.conv.3.running_var' persistent=True
b_features_11_conv_3_num_batches_tracked: BUFFER target='features.11.conv.3.num_batches_tracked' persistent=True
b_features_12_conv_0_1_running_mean: BUFFER target='features.12.conv.0.1.running_mean' persistent=True
b_features_12_conv_0_1_running_var: BUFFER target='features.12.conv.0.1.running_var' persistent=True
b_features_12_conv_0_1_num_batches_tracked: BUFFER target='features.12.conv.0.1.num_batches_tracked' persistent=True
b_features_12_conv_1_1_running_mean: BUFFER target='features.12.conv.1.1.running_mean' persistent=True
b_features_12_conv_1_1_running_var: BUFFER target='features.12.conv.1.1.running_var' persistent=True
b_features_12_conv_1_1_num_batches_tracked: BUFFER target='features.12.conv.1.1.num_batches_tracked' persistent=True
b_features_12_conv_3_running_mean: BUFFER target='features.12.conv.3.running_mean' persistent=True
b_features_12_conv_3_running_var: BUFFER target='features.12.conv.3.running_var' persistent=True
b_features_12_conv_3_num_batches_tracked: BUFFER target='features.12.conv.3.num_batches_tracked' persistent=True
b_features_13_conv_0_1_running_mean: BUFFER target='features.13.conv.0.1.running_mean' persistent=True
b_features_13_conv_0_1_running_var: BUFFER target='features.13.conv.0.1.running_var' persistent=True
b_features_13_conv_0_1_num_batches_tracked: BUFFER target='features.13.conv.0.1.num_batches_tracked' persistent=True
b_features_13_conv_1_1_running_mean: BUFFER target='features.13.conv.1.1.running_mean' persistent=True
b_features_13_conv_1_1_running_var: BUFFER target='features.13.conv.1.1.running_var' persistent=True
b_features_13_conv_1_1_num_batches_tracked: BUFFER target='features.13.conv.1.1.num_batches_tracked' persistent=True
b_features_13_conv_3_running_mean: BUFFER target='features.13.conv.3.running_mean' persistent=True
b_features_13_conv_3_running_var: BUFFER target='features.13.conv.3.running_var' persistent=True
b_features_13_conv_3_num_batches_tracked: BUFFER target='features.13.conv.3.num_batches_tracked' persistent=True
b_features_14_conv_0_1_running_mean: BUFFER target='features.14.conv.0.1.running_mean' persistent=True
b_features_14_conv_0_1_running_var: BUFFER target='features.14.conv.0.1.running_var' persistent=True
b_features_14_conv_0_1_num_batches_tracked: BUFFER target='features.14.conv.0.1.num_batches_tracked' persistent=True
b_features_14_conv_1_1_running_mean: BUFFER target='features.14.conv.1.1.running_mean' persistent=True
b_features_14_conv_1_1_running_var: BUFFER target='features.14.conv.1.1.running_var' persistent=True
b_features_14_conv_1_1_num_batches_tracked: BUFFER target='features.14.conv.1.1.num_batches_tracked' persistent=True
b_features_14_conv_3_running_mean: BUFFER target='features.14.conv.3.running_mean' persistent=True
b_features_14_conv_3_running_var: BUFFER target='features.14.conv.3.running_var' persistent=True
b_features_14_conv_3_num_batches_tracked: BUFFER target='features.14.conv.3.num_batches_tracked' persistent=True
b_features_15_conv_0_1_running_mean: BUFFER target='features.15.conv.0.1.running_mean' persistent=True
b_features_15_conv_0_1_running_var: BUFFER target='features.15.conv.0.1.running_var' persistent=True
b_features_15_conv_0_1_num_batches_tracked: BUFFER target='features.15.conv.0.1.num_batches_tracked' persistent=True
b_features_15_conv_1_1_running_mean: BUFFER target='features.15.conv.1.1.running_mean' persistent=True
b_features_15_conv_1_1_running_var: BUFFER target='features.15.conv.1.1.running_var' persistent=True
b_features_15_conv_1_1_num_batches_tracked: BUFFER target='features.15.conv.1.1.num_batches_tracked' persistent=True
b_features_15_conv_3_running_mean: BUFFER target='features.15.conv.3.running_mean' persistent=True
b_features_15_conv_3_running_var: BUFFER target='features.15.conv.3.running_var' persistent=True
b_features_15_conv_3_num_batches_tracked: BUFFER target='features.15.conv.3.num_batches_tracked' persistent=True
b_features_16_conv_0_1_running_mean: BUFFER target='features.16.conv.0.1.running_mean' persistent=True
b_features_16_conv_0_1_running_var: BUFFER target='features.16.conv.0.1.running_var' persistent=True
b_features_16_conv_0_1_num_batches_tracked: BUFFER target='features.16.conv.0.1.num_batches_tracked' persistent=True
b_features_16_conv_1_1_running_mean: BUFFER target='features.16.conv.1.1.running_mean' persistent=True
b_features_16_conv_1_1_running_var: BUFFER target='features.16.conv.1.1.running_var' persistent=True
b_features_16_conv_1_1_num_batches_tracked: BUFFER target='features.16.conv.1.1.num_batches_tracked' persistent=True
b_features_16_conv_3_running_mean: BUFFER target='features.16.conv.3.running_mean' persistent=True
b_features_16_conv_3_running_var: BUFFER target='features.16.conv.3.running_var' persistent=True
b_features_16_conv_3_num_batches_tracked: BUFFER target='features.16.conv.3.num_batches_tracked' persistent=True
b_features_17_conv_0_1_running_mean: BUFFER target='features.17.conv.0.1.running_mean' persistent=True
b_features_17_conv_0_1_running_var: BUFFER target='features.17.conv.0.1.running_var' persistent=True
b_features_17_conv_0_1_num_batches_tracked: BUFFER target='features.17.conv.0.1.num_batches_tracked' persistent=True
b_features_17_conv_1_1_running_mean: BUFFER target='features.17.conv.1.1.running_mean' persistent=True
b_features_17_conv_1_1_running_var: BUFFER target='features.17.conv.1.1.running_var' persistent=True
b_features_17_conv_1_1_num_batches_tracked: BUFFER target='features.17.conv.1.1.num_batches_tracked' persistent=True
b_features_17_conv_3_running_mean: BUFFER target='features.17.conv.3.running_mean' persistent=True
b_features_17_conv_3_running_var: BUFFER target='features.17.conv.3.running_var' persistent=True
b_features_17_conv_3_num_batches_tracked: BUFFER target='features.17.conv.3.num_batches_tracked' persistent=True
b_features_18_1_running_mean: BUFFER target='features.18.1.running_mean' persistent=True
b_features_18_1_running_var: BUFFER target='features.18.1.running_var' persistent=True
b_features_18_1_num_batches_tracked: BUFFER target='features.18.1.num_batches_tracked' persistent=True
x: USER_INPUT

# outputs
div: USER_OUTPUT
ÇJ
2pkg.torch.export.ExportedProgram.range_constraints{s77: VR[0, int_oo]}B
 